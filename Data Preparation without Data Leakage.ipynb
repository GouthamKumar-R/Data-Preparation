{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied ML process -  Basic steps\n",
    " Step 1: Define Problem.\n",
    "\n",
    " Step 2: Prepare Data.\n",
    "\n",
    " Step 3: Evaluate Models.\n",
    "\n",
    " Step 4: Finalize Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep :  \n",
    "is the transformation of raw data into a form that is more suitable for modeling.\n",
    "\n",
    "is to discover how to best expose the underlying\n",
    "structure of the problem to the learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Data Prep:\n",
    "\n",
    "* Machine learning algorithms require data to be numbers.\n",
    "* Some machine learning algorithms impose requirements on the data.\n",
    "* Statistical noise and errors in the data may need to be corrected.\n",
    "* Complex nonlinear relationships may be teased out of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. Definining the problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " Gather data from the problem domain.\n",
    "\n",
    " Discuss the project with subject matter experts.\n",
    "\n",
    " Select those variables to be used as inputs and outputs for a predictive model.\n",
    "\n",
    " Review the data that has been collected.\n",
    "\n",
    " Summarize the collected data using statistical methods.\n",
    "\n",
    " Visualize the collected data using plots and charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Prep:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " **Data Cleaning:** Identifying and correcting mistakes or errors in the data.\n",
    "\n",
    " **Feature Selection:** Identifying those input variables that are most relevant to the task.\n",
    "\n",
    " **Data Transforms:** Changing the scale or distribution of variables.\n",
    "\n",
    " **Feature Engineering:** Deriving new variables from available data.\n",
    "\n",
    " **Dimensionality Reduction:** Creating compact projections of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Without Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data Leakage: Data leakage refers to a problem where information about the\n",
    "holdout dataset, such as a test or validation dataset, is made available to the model in the\n",
    "training dataset\n",
    "\n",
    "Its like your future personality(test_data) comes and gives you advice on your decision making(model_prediction).\n",
    "\n",
    "> How data leakage happens?\n",
    "\n",
    "We get data leakage by applying data preparation techniques(normalisation or standardization) to the entire dataset. \n",
    "\n",
    "> Solution:\n",
    "\n",
    "The entire pipeline must be prepared only on the training dataset to avoid data leakage. This might include data transforms, but also other techniques such feature selection, dimensionality reduction, feature engineering and more. \n",
    "\n",
    "Steps: \n",
    "1. Split Data.\n",
    "2. Fit Data Preparation on Training Dataset.\n",
    "3. Apply Data Preparation to Train and Test Datasets.\n",
    "4. Evaluate Models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation With Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will evaluate a **logistic regression** model using train and test sets on a synthetic\n",
    "binary classification dataset where the input variables have been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:01:30.900537Z",
     "start_time": "2020-11-06T17:01:27.037419Z"
    }
   },
   "outputs": [],
   "source": [
    "#libraries required for this notebook\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T16:56:56.711608Z",
     "start_time": "2020-11-06T16:56:55.569354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# using make_calssifaiction to create 1000 rows and 20 numerical input features\n",
    "#test classification dataset\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#define dataset\n",
    "X , y = make_classification(n_samples=1000, n_features=20, \n",
    "                            n_informative=15, n_redundant=5, random_state=7)\n",
    "\n",
    "#summarize the ds\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Evaluation With Naive(**incorrect**) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:02:08.383741Z",
     "start_time": "2020-11-06T17:02:08.363797Z"
    }
   },
   "outputs": [],
   "source": [
    "#normalising the ds\n",
    "scalar = MinMaxScaler()\n",
    "\n",
    "X = scalar.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:03:39.289117Z",
     "start_time": "2020-11-06T17:03:39.268158Z"
    }
   },
   "outputs": [],
   "source": [
    "#split into train 67% and test 33%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.33, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:04:27.916581Z",
     "start_time": "2020-11-06T17:04:27.827825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit he model -logistic regression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:08:29.154616Z",
     "start_time": "2020-11-06T17:08:29.139656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model (incorrect data prep) 84.848\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "#evalauate prediction\n",
    "acc = accuracy_score(y_test,yhat)\n",
    "print(\"Accuracy of the model (incorrect data prep) %.3f\" %(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This estimate of model accuracy is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Evaluation With Correct Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:12:25.241155Z",
     "start_time": "2020-11-06T17:12:25.225684Z"
    }
   },
   "outputs": [],
   "source": [
    "#split the dastaset first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                   test_size = 0.33, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:13:07.992893Z",
     "start_time": "2020-11-06T17:13:07.966447Z"
    }
   },
   "outputs": [],
   "source": [
    "#apply necessary transformations separtely\n",
    "\n",
    "# define the scaler\n",
    "scaler = MinMaxScaler()\n",
    "# fit on the training dataset\n",
    "scaler.fit(X_train)\n",
    "# scale the training dataset\n",
    "X_train = scaler.transform(X_train)\n",
    "# scale the test dataset\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:13:23.182520Z",
     "start_time": "2020-11-06T17:13:23.113187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.455\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the correct method and correct estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation With k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation: https://www.datamuni.com/@abhishek/basics-of-cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation procedure changes from simply and incorrectly evaluating just the model\n",
    "to correctly evaluating the entire pipeline of data preparation and model together as a single\n",
    "atomic unit. This can be achieved using the **Pipeline** class. This class takes a list of steps\n",
    "that define the pipeline. Each step in the list is a tuple with two elements. The first element is\n",
    "the name of the step (a string) and the second is the configured object of the step, such as a\n",
    "transform or a model. The model is only supported as the final step, although we can have as\n",
    "many transforms as we like in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:44:27.588287Z",
     "start_time": "2020-11-06T17:44:27.576319Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:48:15.525180Z",
     "start_time": "2020-11-06T17:48:15.498728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scalar', MinMaxScaler()), ('model', LogisticRegression())]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the pipeline\n",
    "\n",
    "steps = list()\n",
    "steps.append(('scalar', MinMaxScaler()))\n",
    "steps.append(('model', LogisticRegression()))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:51:49.832186Z",
     "start_time": "2020-11-06T17:51:45.017551Z"
    }
   },
   "outputs": [],
   "source": [
    "#define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#evaluate the model using cross-validation\n",
    "scores = cross_val_score(pipeline, X, y, \n",
    "                         scoring='accuracy', cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-06T17:53:48.541071Z",
     "start_time": "2020-11-06T17:53:48.519126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-fold CV: 85.433 (3.471)\n"
     ]
    }
   ],
   "source": [
    "#report the performance\n",
    "print(\"Accuracy of k-fold CV: %.3f (%.3f)\" %(mean(scores)*100, std(scores)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API\n",
    "\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
